
###############################################################################
######################## Discovery and identification #########################
###############################################################################

# A dataset should have a concise title.
# Follow the same approach like in giving titles to journal articles.
# Attribute: mandatory
title: 'insert_title'

# A dataset should have a descriptive summary allowing users to understand
# better what the dataset is about and for what it can be used for.
# Attribute: mandatory
summary: 'insert_summary'

# Dataset keywords are similar like journal paper keywords. They highlight
# particularities of the given dataset. Ideally keywords should be sourced
# from control vocabularies which are used by communities.
# Keywords should be provided as a list.
# Attribute: mandatory
keywords: ['insert_keyword_1','insert_keyword_2']


# If some or all keywords are sourced from controlled vocabularies it is a good
# practice to provide a list of the controlled vocabulary URI's from where the
# keywords are sourced from. One example is taxonomy of wind energy topics:
# 'http://data.windenergy.dtu.dk/taxonomy/'
# Attribute: recommended
keywords_vocabulary: ['insert_vocab_URI_1','insert_vocab_URI_2']

# To identify dataset you need to provide a unique & persistent identifier(PID).
# The most obvious candidate for dataset PID is DOI.
# A way to obtain it prior publishing your dataset is to make a DOI reservation.
# Then simply insert the reserved DOI here.
# PIDs should be resolvable.
# Attribute: mandatory
dataset_id: 'insert_PID'

# An URI of the authority body that generated dataset_id
# Attribute: recommended
dataset_id_authority: 'insert_authority_body_URL'

# If dataset is a part of larger data collection is_part_of provides the
# link to the collection. For example, data collection will have PID, such as
# DOI. Therefore, is_part_of will contain that PID.
# Attribute: recommended
is_part_of: 'insert_data_collection_PID'


# If the entire dataset is split to multiple files into multiple files
# is_related_to holds ids of remaining files which make dataset complete.
# The ids of remaining files could be simple as file names.
# Attribute: mandatory (if dataset is split to multiple files)
is_related_to: ['id_file_1', 'id_file_2']

infrastructure_id: # a list of PIDs or URIs (in the worst case infrastructure names) to virtual (models, HPC) or physical (instruments) infrastructure(s) which generated dataset
site: # location where data was created, this is mandatory only for measurements datasets otherwise it can be dropped

creator_name: # an ordered list of FirstName LastName of those whose created the dataset (order according to the level of contributions)
creator_email: # a list of creators' email addresses which follows the order of creator_name
creator_id: # a list of creators' ORCID IDs which follows the order of creator_name, if IDs are missing input empty string
creator_role: # a list of list of roles creators (which follows the order of creator_name list) had in the dataset creation
              # roles are described by CredIT taxonomy: https://like-itn-digitalization.readthedocs.io/en/latest/1_research_lifecycle/#roles-in-lifecycle

contributor_name: # an ordered list of FirstName LastName of those whose contributor to the creation of dataset (order according to the level of contributions)
contributor_email: # an ordered list of contributor email addresses
contributor_id: # an order list of contributor ORCID IDs, if IDs are missing input empty string
contributor_role: # a list of list of roles contributors (which follows the order of creator_name list) had in the dataset creation
                  # roles are described by CredIT taxonomy: https://like-itn-digitalization.readthedocs.io/en/latest/1_research_lifecycle/#roles-in-lifecycle

project_name: # a list of projects which provided funding for the dataset generation
project_id: # a list of project numbers which are identifiable in for example OpenAIER
            # this list is ordered according project_name metadata
project_url: # an ordered list of projects' landing pages


data_mode : # This attribute can take one of following values: "Raw", "Provisional", "Delayed-mode" or "Mixed", where:
            # Raw represents unprocessed data.
            # Provisional data means that some calibrations or editing on data may have been done, but the data is not thought to be fully processed. Refer to the history attribute for more detailed information.
            # Delayed-mode data represents data published after all calibrations and quality control procedures have been applied on the internally recorded or best available original data. This is the best possible version of processed data.
            # Mixed data indicates that the dataset contains data in more than one of the above states.

#############################
## Publication information ##
#############################
license: # licence that specifies usage and distribution of dataset
distribution_statement: # notation that marks dataset according to its security classification to ensure it is circulated only among the authorized recipients.
                        # Potentially the distribution statement could look like this:
                        # Statement A : Approved for public release; distribution is unlimited.
                        # Statement B : Approved for public release; distribution is defined by license.
                        # Statement C : Distribution authorized to (insert authorized entities) only; (fill in reason); (date of determination). Other requests for this document shall be referred to (insert controlling government office).
                        # Statement D : Distribution authorized to (insert authorized entities) and their contractors; (fill in reason); (date of determination). Other requests for this document shall be referred to (insert controlling government office).

publisher_name: # The data publisher's name.
                # The publisher may be an individual or an institution.
publisher_email: # The data publisher's email.
                 # The publisher may be an individual or an institution.
publisher_url: # The data publisher's url.
               # The publisher may be an individual or an institution.
citation: # The citation to be used in publications using the dataset.
update_interval : # in case if dataset is periodically updated (i.e., if this is dynamic dataset) otherwise leave it empty

######################
## Used conventions ##
######################
format_version: # data format version (could be for example "marinet2 NetCDF 2.1" or a resolvable URI)
metadata_schema: # indicates schema used to format/describe metadata, ideally it should be a resolvable URI
conventions: # all conventions used for naming of variables and dimensions, could be a list of resolvable URIs provided as a list

#################################
## Spatio-temporal information ##
#################################
# top level description
feature_type: # Description of the spatio-temporal shape of the data held in the netCDF using a vocabulary specified in CF 1.6.
cdm_data_type: # The Unidata CDM (common data model) data type used by THREDDS. e.g. point, profile, section, station, station_profile, trajectory, grid, radial, swath, image; use Grid for gridded HFR data. Recommended.

# spatial coverage information
reference_system: # Either specify coordinate system in a form of "EPSG:epsg_code" or write "CUSTOM"
coordinate_mapping: # specifies how relative x, y and z coordinates are mapped to absolute one, if reference_system: "CUSTOM" then coordinate_mapping can be dropped
  - name: "x"
    value: # insert to what abs coordinate it is mapped, if reference_system: "EPSG:4326"
           # value will be set to "longitude"
  - name: "y"
    value: # insert to what abs coordinate it is mapped, if reference_system: "EPSG:4326"
           # value will be set to "latitude"
  - name: "z"
    value: # it can be either 'height' or 'depth'

spatial_x_min: # insert min x
spatial_x_max: # insert max x
spatial_x_units: # insert units for x
spatial_x_resolution: # insert resolution for x (optional)

spatial_y_min: # insert min y
spatial_y_max: # insert may y
spatial_y_units: # insert units for y
spatial_y_resolution: # insert resolution for y (optional)

spatial_z_min: # insert min z
spatial_z_max: # insert maz z
spatial_z_units: # insert units for z
spatial_z_resolution: # insert resolution for z (optional)

# temporal coverage information
time_coverage_start: # Start date of the data in UTC. Time must be specified as a string according to the ISO8601 standard: "YYYY-MM-DDThh:mm:ssZ”.
time_coverage_end: # Final date of the data in UTC. Time must be specified as a string according to the ISO8601 standard: "YYYY-MM-DDThh:mm:ssZ”.
time_coverage_resolution: #Interval between records. ISO8601 standard must be used: PnYnMnDTnHnMnS.
time_coverage_duration: #Duration of the time coverage of the data. ISO8601 standard must be used: PnYnMnDTnHnMnS.

#####################
## Data provenance ##
#####################
date_create: # The date on which the data file was created.
              # Version date and time for the data contained in the file. (UTC).
              # Time must be specified as a string according to the ISO8601 standard: "YYYY-MM- DDThh:mm:ssZ”.
date_update: # Timestamp specifying when the contents (i.e. its attributes and/or values)
             # of the file were last changed Time must be specified as a string according to the ISO8601 standard: "YYYY-MM-DDThh:mm:ssZ”
history: # Provides an audit trail for modifications to the original data.
         # It should contain a separate line for each modification, with each line beginning with a timestamp, and including user name,
         # modification name, and modification arguments. The time stamp must be specified as a string according to the ISO8601 standard: "YYYY-MM-DDThh:mm:ssZ”.

processing_level: # Level of processing and quality control applied to data. This probably cannot be generic and it will depend on the community.
                  # For example in case of lidars these levels are:
                  # LEVEL 0: Backscatter signal
                  # LEVEL 1A: Individual Doppler spectra
                  # LEVEL 1B: QC (i.e. filtered) Individual Doppler spectra
                  # LEVEL 1C: QC Averaged Doppler spectra
                  # LEVEL 2A: Estimated non-averaged radial velocity
                  # LEVEL 2B: Estimated non-averaged QC radial velocity
                  # LEVEL 2C: Estimated averaged QC radial velocity
                  # LEVEL 3: Reconstructed wind
                  # LEVEL 4: Extracted flow related parameters (e.g., wind turbine wake width)
